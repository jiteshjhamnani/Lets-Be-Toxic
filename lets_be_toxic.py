# -*- coding: utf-8 -*-
"""lets be toxic

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RhYF7Y3VlVnc1f_2RvUYDKkWT-EWp41m
"""

import nltk
nltk.download('wordnet')
nltk.download('omw-1.4') # Download Open Multilingual WordNet

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import roc_auc_score

train=pd.read_csv('train.csv')
test_df=pd.read_csv('test.csv')

train.head(7)

test_df.head()

label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']

train[label_cols].sum().sort_values(ascending=False).plot(kind='bar', figsize=(8,5), color='tomato')
plt.title("Number of Comments per Toxic Class")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.show()

train['label_sum'] = train[label_cols].sum(axis=1)

train['label_sum'].value_counts().sort_index().plot(kind='bar', color='steelblue')
plt.title("Multi-Label Distribution")
plt.xlabel("Number of Toxic Tags per Comment")
plt.ylabel("Number of Comments")
plt.show()

train['comment_length'] = train['comment_text'].str.len()

plt.figure(figsize=(8,5))
sns.histplot(train['comment_length'], bins=50, kde=True, color='orchid')
plt.title("Distribution of Comment Lengths")
plt.xlabel("Character Length")
plt.ylabel("Frequency")
plt.show()

for label in label_cols:
    print(f"\n\nExample of '{label.upper()}':\n")
    example = train[train[label] == 1]['comment_text'].iloc[3]
    print(example)

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', '', text)
    tokens = text.split()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

train['clean_text'] = train['comment_text'].apply(clean_text)
test_df['clean_text'] = test_df['comment_text'].apply(clean_text)

tfidf = TfidfVectorizer(
    max_features=50000,
    ngram_range=(1,2),
    stop_words='english'
)

X_train = tfidf.fit_transform(train['clean_text'])
X_test = tfidf.transform(test_df['clean_text'])

print("TF-IDF matrix shape (train):", X_train.shape)
print("TF-IDF matrix shape (test):", X_test.shape)

label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
y_train = train[label_cols]

model = OneVsRestClassifier(LogisticRegression(solver='liblinear'))
model.fit(X_train, y_train)

train_preds = model.predict_proba(X_train)

for i, label in enumerate(label_cols):
    score = roc_auc_score(y_train[label], train_preds[:, i])
    print(f"{label}: ROC AUC = {score:.4f}")

mean_auc = roc_auc_score(y_train, train_preds, average='macro')
print(f"\nMean ROC AUC: {mean_auc:.4f}")

test_preds = model.predict_proba(X_test)

custom_text = input("enter a comment :--   ")


custom_text_transformed = tfidf.transform([custom_text])


prediction = model.predict(custom_text_transformed)


print("Predicted toxic probabilities (multi-label):")
for label, score in zip(label_cols, prediction[0]):
    print(f"{label}: {score:.2f}")